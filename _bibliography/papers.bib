---
---
@inproceedings{kim-etal-2023-TSLD,
    abbr={NeurIPS 2023},
    title = "Token-Scaled Logit Distillation for Ternary Weight Generative Language Models",
    author = "Kim, Minsoo  and
      Lee, Sihwa  and
      Lee, Janghwan  and
      Hong, Suk-Jin  and
      Chang, Du-Seong  and
      Sung, Won Yong and
      Choi, Jungwook",
    booktitle = "Thirty-seventh Conference on Neural Information Processing System",
    month = Dec,
    year = "2023",
    abstract = "Generative Language Models (GLMs) have shown impressive performance in tasks such as text generation, understanding, and reasoning. However, the large model size poses challenges for practical deployment. To solve this problem, Quantization-Aware Training (QAT) has become increasingly popular. However, current QAT methods for generative models have resulted in a noticeable loss of accuracy. To counteract this issue, we propose a novel knowledge distillation method specifically designed for GLMs. Our method, called token-scaled logit distillation, prevents overfitting and provides superior learning from the teacher model and ground truth. This research marks the first evaluation of ternary weight quantization-aware training of large-scale GLMs with less than 1.0 degradation in perplexity and no loss of accuracy in a reasoning task.",
    pdf={https://arxiv.org/abs/2308.06744},
    selected={true},
}

@inproceedings{lee-etal-2023enhancing,
abbr={EMNLP 2023},
title={Enhancing Computation Efficiency in Large Language Models through Weight and Activation Quantization},
author={Lee*, Janghwan and Kim*, Minsoo and Baek, Seungcheol and Hwang, Seokjoong and Sung, Wonyong and Choi, Jungwook},
booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (Main Track) to appear},
year={2023},
abstract = "Large Language Models (LLMs) are proficient in natural language processing tasks, but their deployment is often restricted by extensive parameter sizes and computational demands. This paper focuses on post-training quantization (PTQ) in LLMs, specifically 4-bit weight and 8-bit activation (W4A8) quantization, to enhance computational efficiency—a topic less explored compared to weight-only quantization. We present two innovative techniques: activation-quantization-aware scaling (AQAS) and sequence-length-aware calibration (SLAC) to optimize PTQ by considering the combined effects on weights and activations and aligning calibration sequence lengths. Moreover, we introduce dINT, a hybrid data format combining integer and denormal representations, to address the underflow issue in W4A8 quantization, where small values are rounded to zero. Through rigorous evaluations of LLMs, including OPT and LLaMA, we demonstrate that our techniques significantly boost task accuracies to levels comparable with full-precision models. By developing arithmetic units compatible with dINT, we further confirm that our methods yield 2x
 hardware efficiency.",
selected={true},
}


@inproceedings{kim-etal-2023-TI,
    abbr={EACL 2023},
    title = "Teacher Intervention: Improving Convergence of Quantization Aware Training for Ultra-Low Precision Transformers",
    author = "Kim, Minsoo  and
      Shim, Kyuhong  and
      Park, Seongmin  and
      Sung, Wonyong  and
      Choi, Jungwook",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics (Main Track)",
    month = May,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    abstract = "Pre-trained Transformer models such as BERT have shown great success in a wide range of applications, but at the cost of substantial increases in model complexity. Quantization-aware training (QAT) is a promising method to lower the implementation cost and energy consumption. However, aggressive quantization below 2-bit causes considerable accuracy degradation due to unstable convergence, especially when the downstream dataset is not abundant.
This work proposes a proactive knowledge distillation method called \textit{Teacher Intervention} (TI) for fast converging QAT of ultra-low precision pre-trained Transformers. TI intervenes layer-wise signal propagation with the intact signal from the teacher to remove the interference of propagated quantization errors, smoothing loss surface of QAT and expediting the convergence. Furthermore, we propose a \textit{gradual} intervention mechanism to stabilize the recovery of subsections of Transformer layers from quantization. The proposed schemes enable fast convergence of QAT and improve the model accuracy regardless of the diverse characteristics of downstream fine-tuning tasks. We demonstrate that TI consistently achieves superior accuracy with significantly lower fine-tuning iterations on well-known Transformers of natural language processing as well as computer vision compared to the state-of-the-art QAT methods.",
    pdf={https://aclanthology.org/2023.eacl-main.64},
    code="https://github.com/MarsJacobs/ti-kd-qat",
    selected={true},
    poster= {eacl2023_mskim_poster_final_ver.pdf}
}

@inproceedings{kim-etal-2022-understanding,
    abbr={EMNLP 2022},
    title = "Understanding and Improving Knowledge Distillation for Quantization Aware Training of Large Transformer Encoders",
    author = "Kim, Minsoo  and
      Lee, Sihwa  and
      Hong, Suk-Jin  and
      Chang, Du-Seong  and
      Choi, Jungwook",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (Main Track)",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.450",
    pages = "6713--6725",
    abstract = "Knowledge distillation (KD) has been a ubiquitous method for model compression to strengthen the capability of a lightweight model with the transferred knowledge from the teacher. In particular, KD has been employed in quantization-aware training (QAT) of Transformer encoders like BERT to improve the accuracy of the student model with the reduced-precision weight parameters. However, little is understood about which of the various KD approaches best fits the QAT of Transformers. In this work, we provide an in-depth analysis of the mechanism of KD on attention recovery of quantized large Transformers. In particular, we reveal that the previously adopted MSE loss on the attention score is insufficient for recovering the self-attention information. Therefore, we propose two KD methods; attention-map and attention-output losses. Furthermore, we explore the unification of both losses to address task-dependent preference between attention-map and output losses. The experimental results on various Transformer encoder models demonstrate that the proposed KD methods achieve state-of-the-art accuracy for QAT with sub-2-bit weight quantization.",
    selected={true},
    pdf={https://aclanthology.org/2022.emnlp-main.450},
    code="https://github.com/MarsJacobs/kd-qat-large-enc",
    poster= {emnlp2022_mskim_poster_final.pdf}
}


@inproceedings{10.1145/3489517.3530505,
abbr={DAC 2022},
author = {Yu, Joonsang and Park, Junki and Park, Seongmin and Kim, Minsoo and Lee, Sihwa and Lee, Dong Hyun and Choi, Jungwook},
title = {NN-LUT: Neural Approximation of Non-Linear Operations for Efficient Transformer Inference},
year = {2022},
isbn = {9781450391429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489517.3530505},
doi = {10.1145/3489517.3530505},
abstract = {Non-linear operations such as GELU, Layer normalization, and Soft-max are essential yet costly building blocks of Transformer models. Several prior works simplified these operations with look-up tables or integer computations, but such approximations suffer inferior accuracy or considerable hardware cost with long latency. This paper proposes an accurate and hardware-friendly approximation framework for efficient Transformer inference. Our framework employs a simple neural network as a universal approximator with its structure equivalently transformed into a Look-up table(LUT). The proposed framework called Neural network generated LUT(NN-LUT) can accurately replace all the non-linear operations in popular BERT models with significant reductions in area, power consumption, and latency.},
booktitle = {Proceedings of the 59th ACM/IEEE Design Automation Conference},
pages = {577–582},
numpages = {6},
keywords = {look-up table, neural network, non-linear function, transformer},
location = {San Francisco, California},
series = {DAC '22},
selected={true},
pdf = {https://arxiv.org/abs/2112.02191}
}