---
layout: about
title: about
permalink: /
subtitle: Hanyang University. Seoul, South Korea. minsoo2333@hanyang.ac.kr

profile:
  align: right
  image: mskim_230127_idcard.jpg
  image_circular: false # crops the image to make it circular
  
news: false  # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
social: false  # includes social icons at the bottom of the page
---

Hi! I am Minsoo Kim, first year Ph.D. student in the Artificial Intelligence Hardware & Algorithm lab at [Hanyang University](https://www.hanyang.ac.kr/web/eng). I am fortunate to be advised by professor [Jungwook Choi](https://jchoi-hyu.github.io/). 

My core research focuses on understanding the effects of Quantization in language models across diverse NLP tasks. Using these insights, I've actively applied **Knowledge Distillation** in **Quantization Aware Training (QAT)** to enhance the robust performance of quantized language models, aimed for efficient inference.

Currently, my research interest lies in the area of **LLM Fine-Tuning with Quantization** with memory efficient Knowledge Distillation. Not limited to Quantization, I also hold a deep interest in techniques for fine-tuning smaller LLMs to unleash the potential of **specialized LLMs**, in the direction of democratizing LLMs for broader accessibility.