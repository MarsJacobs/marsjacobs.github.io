---
layout: about
title: about
permalink: /
subtitle: Hanyang University. Seoul, South Korea. minsoo2333@hanyang.ac.kr

profile:
  align: right
  image: mskim_230127_idcard.jpg
  image_circular: false # crops the image to make it circular
  
news: false  # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
social: false  # includes social icons at the bottom of the page
---

Hi! I am Minsoo Kim, first year Ph.D. student in the Artificial Intelligence Hardware & Algorithm lab at [Hanyang University](https://www.hanyang.ac.kr/web/eng). I am fortunate to be advised by professor [Jungwook Choi](https://jchoi-hyu.github.io/). My main research lies in conducting in-depth analysis of the **various effects of Quantization within language models** across different NLP tasks. Drawing from these insightful observations, I've been proactive in leveraging **Knowledge Distillation** with **Quantization Aware Training (QAT)** to enhance the robust performance of quantized language models, aimed for efficient inference. 

Currently, my research interest lies in the area of **LLM Fine-Tuning with Quantization** with memory efficient **Knowledge Distillation**. Not limited to Quantization, I also hold a deep interest in techniques for fine-tuning smaller LLMs to unleash the potential of specialized LLMs, all in the direction of democratizing LLMs for broader accessibility.